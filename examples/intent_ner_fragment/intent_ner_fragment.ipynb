{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 1\n",
    "\n",
    "**Tasks :- Intent Detection, NER, Fragment Detection**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``Intent Detection`` :- This is a single sentence classification task where an `intent` specifies which class the data sample belongs to. Intent detection is one of the fundamental components for conversational system as it gives a broad understand of the category/domain the sentence/query belongs to. \n",
    "\n",
    "``NER`` :- This is a Named Entity Recognition/ Sequence Labelling/ Slot filling task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\". NER\n",
    "\n",
    "``Fragment Detection`` :- This is modeled as a single sentence classification task which detects whether a sentence is incomplete (fragment) or not (non-fragment). This is a very useful piece in conversational system as knowing if a query/sentence is incomplete beforehand aids in \n",
    "\n",
    "\n",
    "**Data** :- In this example, we are using the <a href= \"https://snips-nlu.readthedocs.io/en/latest/dataset.html\">SNIPS</a> data for intent and entity detection. For the sake of simplicity, we provide \n",
    "the data in simpler form under ``snips_data`` directory taken from <a href = \"https://github.com/LeePleased/StackPropagation-SLU/tree/master/data/snips\">here</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "The data is present in *BIO* format where each word in a sentence is tagged with corresponding entity. \n",
    "Sentences are separated by \\\" \" and at the end of each sentence, intent class to which the sentence belongs is mentioned. We already provide a sample transformation function ``snips_intent_ner_to_tsv`` to convert this data to separate intent and NER tsv data files.\n",
    "Fragment detection data is generated from intent detection data created using the transform function\n",
    "``create_fragment_detection_tsv``. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_snips.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: snips_intent_ner_to_tsv\n",
    "  read_file_names:\n",
    "    - snips_train.txt\n",
    "    - snips_dev.txt\n",
    "    - snips_test.txt\n",
    "  read_dir: snips_data\n",
    "  save_dir: ../../data\n",
    "  \n",
    "transform2:\n",
    "  transform_func: create_fragment_detection_tsv\n",
    "  read_file_names:\n",
    "    - intent_snips_train.tsv\n",
    "    - intent_snips_dev.tsv\n",
    "    - intent_snips_test.tsv\n",
    "  read_dir: ../../data\n",
    "  save_dir: ../../data\n",
    "  \n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making data from file snips_train.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "NER File Written at ../../data\n",
      "Intent File Written at ../../data\n",
      "Created NER label map from train file snips_train.txt\n",
      "{'O': 0, 'B-artist': 1, 'B-album': 2, 'B-service': 3, 'I-service': 4, 'B-entity_name': 5, 'I-entity_name': 6, 'B-playlist': 7, 'I-playlist': 8, 'B-object_select': 9, 'B-object_type': 10, 'B-rating_value': 11, 'B-best_rating': 12, 'B-music_item': 13, 'B-track': 14, 'I-track': 15, 'I-artist': 16, 'B-playlist_owner': 17, 'B-year': 18, 'B-sort': 19, 'B-movie_name': 20, 'I-movie_name': 21, 'B-party_size_number': 22, 'B-state': 23, 'B-city': 24, 'B-timeRange': 25, 'I-timeRange': 26, 'B-object_part_of_series_type': 27, 'I-object_type': 28, 'B-movie_type': 29, 'B-spatial_relation': 30, 'I-spatial_relation': 31, 'B-geographic_poi': 32, 'I-geographic_poi': 33, 'B-restaurant_type': 34, 'I-city': 35, 'B-party_size_description': 36, 'I-party_size_description': 37, 'B-object_location_type': 38, 'I-object_location_type': 39, 'B-object_name': 40, 'I-object_name': 41, 'I-movie_type': 42, 'B-rating_unit': 43, 'I-sort': 44, 'B-location_name': 45, 'I-location_name': 46, 'B-current_location': 47, 'I-current_location': 48, 'I-playlist_owner': 49, 'B-served_dish': 50, 'B-country': 51, 'B-condition_temperature': 52, 'B-poi': 53, 'I-poi': 54, 'B-condition_description': 55, 'B-genre': 56, 'B-restaurant_name': 57, 'I-restaurant_name': 58, 'I-state': 59, 'I-served_dish': 60, 'B-cuisine': 61, 'I-album': 62, 'I-country': 63, 'B-facility': 64, 'I-facility': 65, 'I-cuisine': 66, 'I-music_item': 67, 'I-object_select': 68, 'I-restaurant_type': 69, 'I-object_part_of_series_type': 70, 'I-genre': 71}\n",
      "label Map NER written at ../../data/ner_snips_train_label_map.joblib\n",
      "Created Intent label map from train file snips_train.txt\n",
      "{'PlayMusic': 0, 'AddToPlaylist': 1, 'RateBook': 2, 'SearchScreeningEvent': 3, 'BookRestaurant': 4, 'GetWeather': 5, 'SearchCreativeWork': 6}\n",
      "label Map Intent written at ../../data/int_snips_train_label_map.joblib\n",
      "Making data from file snips_dev.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at ../../data\n",
      "Intent File Written at ../../data\n",
      "Making data from file snips_test.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at ../../data\n",
      "Intent File Written at ../../data\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 743.63it/s]\n",
      "number of fragment samples :  26506\n",
      "number of non-fragment samples :  13084\n",
      "writing fragment file for intent_snips_train.tsv at ../../data\n",
      "0it [00:00, ?it/s]\n",
      "number of fragment samples :  2176\n",
      "number of non-fragment samples :  700\n",
      "writing fragment file for intent_snips_dev.tsv at ../../data\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 5090.17it/s]\n",
      "number of fragment samples :  2171\n",
      "number of non-fragment samples :  700\n",
      "writing fragment file for intent_snips_test.tsv at ../../data\n"
     ]
    }
   ],
   "source": [
    "!python ../../data_transformations.py \\\n",
    "    --transform_file 'transform_file_snips.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "Here we are training the three tasks together for demonstration. This means we will have a single\n",
    "multi-task model capable of performing on all the three tasks. You can also train the tasks separately \n",
    "by mentioning single tasks in task file.\n",
    "\n",
    "For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for multiple tasks - intent detection, NER and fragment detection. The file is already created at ``tasks_file_snips.yml``\n",
    "\n",
    "```\n",
    "ner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.3\n",
    "  label_map_or_file: ../../data/ner_snips_train_label_map.joblib\n",
    "  metrics:\n",
    "  - snips_f1_score\n",
    "  - snips_precision\n",
    "  - snips_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_snips_train.tsv\n",
    "  - ner_snips_dev.tsv\n",
    "  - ner_snips_test.tsv\n",
    "\n",
    "intent:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.3\n",
    "    label_map_or_file: ../../data/int_snips_train_label_map.joblib\n",
    "    metrics:\n",
    "    - classification_accuracy\n",
    "    loss_type: CrossEntropyLoss\n",
    "    task_type: SingleSenClassification\n",
    "    file_names:\n",
    "    - int_snips_train.tsv\n",
    "    - int_snips_dev.tsv\n",
    "    - int_snips_test.tsv\n",
    "\n",
    "    \n",
    "fragment:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    class_num: 2\n",
    "    metrics:\n",
    "    - classification_accuracy\n",
    "    loss_type: CrossEntropyLoss\n",
    "    task_type: SingleSenClassification\n",
    "    file_names:\n",
    "    - fragment_snips_train.tsv\n",
    "    - fragment_snips_dev.tsv\n",
    "    - fragment_snips_test.tsv\n",
    "```\n",
    "\n",
    "Following command can be used to run the data preparation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "task object created from task file...\n",
      "bert model tokenizer loaded for config bert-base-uncased\n",
      "Loading raw data for task ner from ../../data/ner_snips_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  13083\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for ner. File saved at ../../data/bert-base-uncased_prepared_data/ner_snips_train.json\n",
      "Loading raw data for task ner from ../../data/ner_snips_dev.tsv\n",
      "Processing Started...\n",
      "Data Size:  700\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for ner. File saved at ../../data/bert-base-uncased_prepared_data/ner_snips_dev.json\n",
      "Loading raw data for task ner from ../../data/ner_snips_test.tsv\n",
      "Processing Started...\n",
      "Data Size:  699\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for ner. File saved at ../../data/bert-base-uncased_prepared_data/ner_snips_test.json\n",
      "Loading raw data for task intent from ../../data/int_snips_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  13084\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=4361.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for intent. File saved at ../../data/bert-base-uncased_prepared_data/int_snips_train.json\n",
      "Loading raw data for task intent from ../../data/int_snips_dev.tsv\n",
      "Processing Started...\n",
      "Data Size:  700\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for intent. File saved at ../../data/bert-base-uncased_prepared_data/int_snips_dev.json\n",
      "Loading raw data for task intent from ../../data/int_snips_test.tsv\n",
      "Processing Started...\n",
      "Data Size:  700\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=233.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for intent. File saved at ../../data/bert-base-uncased_prepared_data/int_snips_test.json\n",
      "Loading raw data for task fragdetect from ../../data/fragment_snips_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  35980\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=11993.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=11993.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=11993.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for fragdetect. File saved at ../../data/bert-base-uncased_prepared_data/fragment_snips_train.json\n",
      "Loading raw data for task fragdetect from ../../data/fragment_snips_dev.tsv\n",
      "Processing Started...\n",
      "Data Size:  3761\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=1253.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=1253.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=1253.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for fragdetect. File saved at ../../data/bert-base-uncased_prepared_data/fragment_snips_dev.json\n",
      "Loading raw data for task fragdetect from ../../data/fragment_snips_test.tsv\n",
      "Processing Started...\n",
      "Data Size:  2909\n",
      "number of threads:  3\n",
      "HBox(children=(FloatProgress(value=0.0, max=969.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=969.0), HTML(value='')))\n",
      "HBox(children=(FloatProgress(value=0.0, max=969.0), HTML(value='')))\n",
      "\n",
      "\n",
      "\n",
      "Data Processing done for fragdetect. File saved at ../../data/bert-base-uncased_prepared_data/fragment_snips_test.json\n"
     ]
    }
   ],
   "source": [
    "!python ../../data_preparation.py \\\n",
    "    --task_file 'tasks_file_snips.yml' \\\n",
    "    --data_dir '../../data' \\\n",
    "    --max_seq_len 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 3 Running train\n",
    "\n",
    "Following command will start the training for the tasks. The log file reporting the loss, metrics and the tensorboard logs will be present in a time-stamped directory. For demonstration, we've put up sample logs under ``train_logs`` directory.\n",
    "\n",
    "For knowing more details about the train process, refer to <a href= \"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-train\">running training</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python ../../train.py \\\n",
    "    --data_dir '../../data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_snips.yml' \\\n",
    "    --out_dir 'snips_intent_ner_fragment_bert_base' \\\n",
    "    --epochs 3 \\\n",
    "    --train_batch_size 16 \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --grad_accumulation_steps 2 \\\n",
    "    --log_per_updates 50 \\\n",
    "    --eval_while_train True \\\n",
    "    --test_while_train True \\\n",
    "    --max_seq_len 50 \\\n",
    "    --silent True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = inferPipeline('snips_intent_ner_bert_base/', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.infer([ [],\n",
    "            ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
